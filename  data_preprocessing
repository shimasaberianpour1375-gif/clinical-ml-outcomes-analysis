{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNftQ5lTuP+63aBNpyl83/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimasaberianpour1375-gif/clinical-ml-outcomes-analysis/blob/main/%20data_preprocessing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For handling imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# For Deep Learning (Keras with TensorFlow backend)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# For SHAP\n",
        "import shap\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_PATH = 'physio.csv'\n",
        "TARGET_VARIABLE = 'pain'\n",
        "\n"
      ],
      "metadata": {
        "id": "cEDljdFMqVq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfHbovM2qTq_"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Load the Dataset ---\n",
        "print(\"--- Step 1: Loading Dataset ---\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"Dataset loaded successfully from {DATA_PATH}.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {DATA_PATH} not found\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 2: Initial Data Inspection\n",
        "\n",
        "# all columns to be used initially before dropping anything\n",
        "all_data_columns = df.columns.tolist()\n",
        "\n",
        "numerical_cols_for_imputation = df.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols_for_imputation = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# The target variable should not be imputed with features data\n",
        "if TARGET_VARIABLE in numerical_cols_for_imputation:\n",
        "    numerical_cols_for_imputation.remove(TARGET_VARIABLE)\n",
        "if TARGET_VARIABLE in categorical_cols_for_imputation:\n",
        "    categorical_cols_for_imputation.remove(TARGET_VARIABLE)\n",
        "\n",
        "# **********Handle missing values for numerical and categorical columns\n",
        "for col in numerical_cols_for_imputation:\n",
        "    if df[col].isnull().any():\n",
        "        median_val = df[col].median()\n",
        "        df[col].fillna(median_val, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with median: {median_val}\")\n",
        "\n",
        "for col in categorical_cols_for_imputation:\n",
        "    if df[col].isnull().any():\n",
        "        mode_val = df[col].mode()[0]\n",
        "        df[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with mode: {mode_val}\")\n",
        "\n",
        "# --- Step 3: Define Features (X) and Target (y)\n",
        "print(\"\\n--- Step 3: Defining Features (X) and Target (y)  ---\")\n",
        "\n",
        "#  independent variables for 'pain' target\n",
        "selected_independent_variables = [\n",
        "    'gender', 'age', 'symptoms', 'treatments', 'episodes', 'baseline'\n",
        "]\n",
        "\n",
        "# ******* columns exist in the DataFrame before selecting\n",
        "missing_cols = [col for col in selected_independent_variables if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Error: variables are not found in the dataset: {missing_cols}\")\n",
        "    exit()\n",
        "\n",
        "X = df[selected_independent_variables]\n",
        "y = df[TARGET_VARIABLE]\n",
        "\n",
        "print(f\"\\nFeatures (X) shape with selected independent variables: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "\n",
        "# --- Step 4: Separate Feature Types for Preprocessing\n",
        "print(\"\\n--- Step 4: Separating Feature Types for Preprocessing ---\")\n",
        "\n",
        "# Updated lists for numerical and categorical features\n",
        "numerical_features = ['age']\n",
        "categorical_features = ['gender', 'symptoms', 'treatments', 'episodes', 'baseline']\n",
        "\n",
        "# *******************************Verify that all selected columns in X are accounted\n",
        "all_processed_features = set(numerical_features + categorical_features)\n",
        "X_columns_set = set(X.columns.tolist())\n",
        "\n",
        "if all_processed_features != X_columns_set:\n",
        "    print(\"\\n--- WARNING: Mismatch in selected feature lists ---\")\n",
        "    print(f\"Columns in X but not in feature lists: {X_columns_set - all_processed_features}\")\n",
        "    print(f\"Features listed but not in X: {all_processed_features - X_columns_set}\")\n",
        "    print(\"Please double-check the numerical/categorical lists against your explicitly selected X columns.\")\n",
        "else:\n",
        "    print(f\"\\nNumerical Features identified: {numerical_features}\")\n",
        "    print(f\"Categorical Features identified: {categorical_features}\")\n",
        "\n",
        "# --- Step 5:Preprocessing Pipeline ---\n",
        "print(\"\\n--- Step 5: Preprocessing Pipeline ---\")\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop=\"first\",handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created.\")\n",
        "\n",
        "# --- Step 6: Split Data into Training and Testing Sets ---\n",
        "print(\"\\n--- Step 6: Splitting data into Training and Testing sets ---\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# --- Step 7: Apply Preprocessing to Training and Testing Sets ---\n",
        "print(\"\\n--- Step 7: Applying Preprocessing to Training and Testing Sets ---\")\n",
        "# Original: X_train_processed = preprocessor.fit_transform(X_train)\n",
        "# Original: X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# MODIFIED: Convert to dense arrays immediately after preprocessing\n",
        "X_train_processed = preprocessor.fit_transform(X_train).toarray() # Add .toarray()\n",
        "X_test_processed = preprocessor.transform(X_test).toarray()     # Add .toarray()\n",
        "\n",
        "try:\n",
        "    ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
        "    processed_feature_names = numerical_features + list(ohe_feature_names)\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not retrieve feature names after one-hot encoding: {e}. Using generic names.\")\n",
        "    processed_feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n",
        "\n",
        "print(f\"X_train_processed shape: {X_train_processed.shape}\")\n",
        "print(f\"X_test_processed shape: {X_test_processed.shape}\")\n",
        "\n",
        "print(\"\\n--- Pre-ML Steps Complete and Data Ready for Modeling ---\")\n",
        "results = {}\n",
        "models = {}\n"
      ]
    }
  ]
}