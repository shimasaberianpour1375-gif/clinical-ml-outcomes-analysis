{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VQ8qi3KA5sJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Step 22: Feature Importances for All Models ---\n",
        "print(\"\\n--- Step 22: Feature Importances for All Models ---\")\n",
        "\n",
        "feature_importance_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            importance_vals = model.feature_importances_\n",
        "        elif hasattr(model, \"coef_\"):\n",
        "            importance_vals = np.abs(model.coef_[0])  # Logistic Regression\n",
        "        else:\n",
        "            # permutation importance\n",
        "            perm_result = permutation_importance(\n",
        "                model, X_test_processed, y_test,\n",
        "                n_repeats=10, random_state=42, n_jobs=-1\n",
        "            )\n",
        "            importance_vals = perm_result.importances_mean\n",
        "\n",
        "        feature_importance_results[name] = pd.DataFrame({\n",
        "            \"Feature\": processed_feature_names,\n",
        "            f\"Importance_{name}\": importance_vals\n",
        "        }).sort_values(by=f\"Importance_{name}\", ascending=False)\n",
        "\n",
        "        print(f\"\\nTop 10 Features for {name}:\")\n",
        "        print(feature_importance_results[name].head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping feature importance for {name}: {e}\")\n",
        "\n",
        "# --- Step 23: Combined Feature Importance Plot (Grouped Bars) ---\n",
        "print(\"\\n--- Step 23: Combined Feature Importance Plot ---\")\n",
        "\n",
        "# all importance tables\n",
        "merged_importances = feature_importance_results[list(feature_importance_results.keys())[0]][[\"Feature\"]]\n",
        "\n",
        "for name, df_imp in feature_importance_results.items():\n",
        "    merged_importances = merged_importances.merge(df_imp, on=\"Feature\", how=\"left\")\n",
        "\n",
        "# --- NEW: Scale all importance values per model to [0,1] ---\n",
        "scaler = MinMaxScaler()\n",
        "for col in merged_importances.columns:\n",
        "    if col != \"Feature\":\n",
        "        merged_importances[col] = scaler.fit_transform(merged_importances[[col]])\n",
        "\n",
        "# Melt for plotting\n",
        "merged_melted = pd.melt(\n",
        "    merged_importances,\n",
        "    id_vars=\"Feature\",\n",
        "    var_name=\"Model\",\n",
        "    value_name=\"Importance\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.barplot(data=merged_melted, x=\"Feature\", y=\"Importance\", hue=\"Model\")\n",
        "plt.title(\"Top Normalized Features across Models\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- Step 24: Calibration Curves ---\")\n",
        "\n",
        "# Define 12 distinct colors\n",
        "colors = [\n",
        "    \"#1f77b4\" ,\"#7f7788\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\",\n",
        "    \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\", \"#aec7e8\", \"#fcbb70\"\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i, (name, y_prob) in enumerate(prob_maps.items()):\n",
        "    color = colors[i % len(colors)]  # cycle colors if more than 12\n",
        "    try:\n",
        "        CalibrationDisplay.from_predictions(\n",
        "            y_test, y_prob, n_bins=10, strategy='uniform',\n",
        "            name=name, ax=plt.gca(), color=color\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping calibration for {name}: {e}\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly Calibrated\")\n",
        "plt.title(\"Calibration Curves for All Models\")\n",
        "plt.xlabel(\"Mean Predicted Probability\")\n",
        "plt.ylabel(\"Fraction of Positives\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yG_1uICVq4Lj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}