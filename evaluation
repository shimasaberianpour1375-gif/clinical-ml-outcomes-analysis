{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn8TepR8f3k89n1isKVA+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimasaberianpour1375-gif/clinical-ml-outcomes-analysis/blob/main/evaluation\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15EAwDrWqtRO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 16: Display All Model Results ---\n",
        "print(\"\\n--- Step 16: Final Model Performance Benchmarking  ---\")\n",
        "results_df_final = pd.DataFrame(results).T\n",
        "print(results_df_final.round(3))\n",
        "\n",
        "# --- Step 17: Visualize ROC Curves  ---\n",
        "print(\"\\n--- Step 17: Visualizing ROC Curves ---\")\n",
        "plt.figure(figsize=(18, 12))\n",
        "lw = 2\n",
        "prob_maps = {\n",
        "    'Logistic Regression': y_prob_lr,\n",
        "    'Random Forest': y_prob_rf,\n",
        "    'XGBoost': y_prob_xgb,\n",
        "    'SVM': y_prob_svm,\n",
        "    'Neural Network (MLP)': y_prob_mlp,\n",
        "    'KNN': y_prob_knn,\n",
        "    'Naive Bayes': y_prob_gnb,\n",
        "    'Voting Classifier': y_prob_voting,\n",
        "    'Tuned Random Forest': y_prob_tuned_rf,\n",
        "    'Tuned XGBoost': y_prob_tuned_xgb,\n",
        "    'RF + SMOTE': y_prob_smote_rf,\n",
        "    'Deep Learning (Keras)': y_prob_keras,\n",
        "    'Stacking Classifier': y_prob_stk\n",
        "}\n",
        "\n",
        "for name, y_prob in prob_maps.items():\n",
        "    if y_prob is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        auc_score = results[name]['AUC-ROC']\n",
        "        plt.plot(fpr, tpr, lw=lw, label=f'{name} (AUC = {auc_score:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Pain Prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show() # Display the plot directly\n",
        "print(\"ROC Curve displayed above.\")\n",
        "\n",
        "# --- Step 18: Generate and Display Confusion Matrices  ---\n",
        "print(\"\\n--- Step 18: Generating Confusion Matrices ---\")\n",
        "labels = np.unique(y_test)\n",
        "\n",
        "num_models_to_plot = len(results)\n",
        "cols = 4\n",
        "rows = (num_models_to_plot + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "pred_maps = {\n",
        "    'Logistic Regression': y_pred_lr,\n",
        "    'Random Forest': y_pred_rf,\n",
        "    'XGBoost': y_pred_xgb,\n",
        "    'SVM': y_pred_svm,\n",
        "    'Neural Network (MLP)': y_pred_mlp,\n",
        "    'KNN': y_pred_knn,\n",
        "    'Naive Bayes': y_pred_gnb,\n",
        "    'Voting Classifier': y_pred_voting,\n",
        "    'Tuned Random Forest': y_pred_tuned_rf,\n",
        "    'Tuned XGBoost': y_pred_tuned_xgb,\n",
        "    'RF + SMOTE': y_pred_smote_rf,\n",
        "    'Deep Learning (Keras)': y_pred_keras,\n",
        "    'Stacking Classifier': y_pred_stk\n",
        "}\n",
        "\n",
        "for i, (name, y_pred) in enumerate(pred_maps.items()):\n",
        "    if y_pred is not None and i < len(axes):\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "        disp.plot(cmap=plt.cm.Blues, ax=axes[i], values_format='d')\n",
        "        axes[i].set_title(f'CM: {name}')\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show() # Display the plot directly\n",
        "\n"
      ],
      "metadata": {
        "id": "a9EqV_TGfaao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 19: Basic Model Interpretability ---\n",
        "print(\"\\n--- Step 19: Basic Model Interpretability  ---\")\n",
        "\n",
        "if 'processed_feature_names' not in locals() or not processed_feature_names:\n",
        "    print(\"Feature names not \")\n",
        "else:\n",
        "    print(\"\\nLogistic Regression Coefficients (Top 10 by Absolute Magnitude):\")\n",
        "    lr_coefficients = pd.DataFrame({'Feature': processed_feature_names, 'Coefficient': log_reg_model.coef_[0]})\n",
        "    lr_coefficients['Absolute_Coefficient'] = np.abs(lr_coefficients['Coefficient'])\n",
        "    lr_coefficients = lr_coefficients.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "    print(lr_coefficients.head(10))\n",
        "\n",
        "    print(\"\\nRandom Forest Feature Importance (Top 10 - using original RF):\")\n",
        "    rf_feature_importances = pd.DataFrame({'Feature': processed_feature_names, 'Importance': rf_model.feature_importances_})\n",
        "    rf_feature_importances = rf_feature_importances.sort_values(by='Importance', ascending=False)\n",
        "    print(rf_feature_importances.head(10))\n",
        "\n",
        "    print(\"\\nXGBoost Feature Importance (Top 10):\")\n",
        "    xgb_feature_importances = pd.DataFrame({'Feature': processed_feature_names, 'Importance': xgb_model.feature_importances_})\n",
        "    xgb_feature_importances = xgb_feature_importances.sort_values(by='Importance', ascending=False)\n",
        "    print(xgb_feature_importances.head(10))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Step 20: Model Interpretability with SHAP (for Tuned XGBoost)  ---\n",
        "print(\"\\n--- Step 20:  Model Interpretability with SHAP (Tuned XGBoost) ---\")\n",
        "\n",
        "if not isinstance(processed_feature_names, list):\n",
        "    processed_feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n",
        "\n",
        "X_test_processed_df = pd.DataFrame(X_test_processed, columns=processed_feature_names)\n",
        "\n",
        "try:\n",
        "    explainer = shap.TreeExplainer(models['Tuned XGBoost'])\n",
        "    shap_values = explainer.shap_values(X_test_processed_df)\n",
        "\n",
        "    print(\"\\n SHAP Summary Plot (Global Feature Importance - Bar Plot)...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values, X_test_processed_df, plot_type=\"bar\", show=False)\n",
        "    plt.title('SHAP Feature Importance for Tuned XGBoost ')\n",
        "    plt.tight_layout()\n",
        "    plt.show() # Display plot\n",
        "    print(\"SHAP Summary Bar Plot displayed above.\")\n",
        "\n",
        "    print(\"\\nGenerating SHAP Summary Plot (Beeswarm Plot)...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values, X_test_processed_df, show=False) # Beeswarm plot\n",
        "    plt.title('SHAP Summary Plot (Beeswarm) for Tuned XGBoost ')\n",
        "    plt.tight_layout()\n",
        "    plt.show() #  plot\n",
        "\n",
        "    if 'age' in processed_feature_names:\n",
        "        print(\"\\nGenerating SHAP Dependence Plot for 'age'...\")\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        shap.dependence_plot(\"age\", shap_values, X_test_processed_df, show=False)\n",
        "        plt.title('SHAP Dependence Plot for Age ')\n",
        "        plt.tight_layout()\n",
        "        plt.show() # plot\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during SHAP analysis: {e}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1LDEk3UGqye5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Step 22: Feature Importances for All Models ---\n",
        "print(\"\\n--- Step 22: Feature Importances for All Models ---\")\n",
        "\n",
        "feature_importance_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            importance_vals = model.feature_importances_\n",
        "        elif hasattr(model, \"coef_\"):\n",
        "            importance_vals = np.abs(model.coef_[0])  # Logistic Regression\n",
        "        else:\n",
        "            # permutation importance\n",
        "            perm_result = permutation_importance(\n",
        "                model, X_test_processed, y_test,\n",
        "                n_repeats=10, random_state=42, n_jobs=-1\n",
        "            )\n",
        "            importance_vals = perm_result.importances_mean\n",
        "\n",
        "        feature_importance_results[name] = pd.DataFrame({\n",
        "            \"Feature\": processed_feature_names,\n",
        "            f\"Importance_{name}\": importance_vals\n",
        "        }).sort_values(by=f\"Importance_{name}\", ascending=False)\n",
        "\n",
        "        print(f\"\\nTop 10 Features for {name}:\")\n",
        "        print(feature_importance_results[name].head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping feature importance for {name}: {e}\")\n",
        "\n",
        "# --- Step 23: Combined Feature Importance Plot (Grouped Bars) ---\n",
        "print(\"\\n--- Step 23: Combined Feature Importance Plot ---\")\n",
        "\n",
        "# all importance tables\n",
        "merged_importances = feature_importance_results[list(feature_importance_results.keys())[0]][[\"Feature\"]]\n",
        "\n",
        "for name, df_imp in feature_importance_results.items():\n",
        "    merged_importances = merged_importances.merge(df_imp, on=\"Feature\", how=\"left\")\n",
        "\n",
        "# --- NEW: Scale all importance values per model to [0,1] ---\n",
        "scaler = MinMaxScaler()\n",
        "for col in merged_importances.columns:\n",
        "    if col != \"Feature\":\n",
        "        merged_importances[col] = scaler.fit_transform(merged_importances[[col]])\n",
        "\n",
        "# Melt for plotting\n",
        "merged_melted = pd.melt(\n",
        "    merged_importances,\n",
        "    id_vars=\"Feature\",\n",
        "    var_name=\"Model\",\n",
        "    value_name=\"Importance\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.barplot(data=merged_melted, x=\"Feature\", y=\"Importance\", hue=\"Model\")\n",
        "plt.title(\"Top Normalized Features across Models\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- Step 24: Calibration Curves ---\")\n",
        "\n",
        "# Define 12 distinct colors\n",
        "colors = [\n",
        "    \"#1f77b4\" ,\"#7f7788\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\",\n",
        "    \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\", \"#aec7e8\", \"#fcbb70\"\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i, (name, y_prob) in enumerate(prob_maps.items()):\n",
        "    color = colors[i % len(colors)]  # cycle colors if more than 12\n",
        "    try:\n",
        "        CalibrationDisplay.from_predictions(\n",
        "            y_test, y_prob, n_bins=10, strategy='uniform',\n",
        "            name=name, ax=plt.gca(), color=color\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping calibration for {name}: {e}\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly Calibrated\")\n",
        "plt.title(\"Calibration Curves for All Models\")\n",
        "plt.xlabel(\"Mean Predicted Probability\")\n",
        "plt.ylabel(\"Fraction of Positives\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yG_1uICVq4Lj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}