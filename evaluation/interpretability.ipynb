{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trKWmsmq-8BV"
      },
      "outputs": [],
      "source": [
        "# --- Step 19: Basic Model Interpretability ---\n",
        "print(\"\\n--- Step 19: Basic Model Interpretability  ---\")\n",
        "\n",
        "if 'processed_feature_names' not in locals() or not processed_feature_names:\n",
        "    print(\"Feature names not \")\n",
        "else:\n",
        "    print(\"\\nLogistic Regression Coefficients (Top 10 by Absolute Magnitude):\")\n",
        "    lr_coefficients = pd.DataFrame({'Feature': processed_feature_names, 'Coefficient': log_reg_model.coef_[0]})\n",
        "    lr_coefficients['Absolute_Coefficient'] = np.abs(lr_coefficients['Coefficient'])\n",
        "    lr_coefficients = lr_coefficients.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "    print(lr_coefficients.head(10))\n",
        "\n",
        "    print(\"\\nRandom Forest Feature Importance (Top 10 - using original RF):\")\n",
        "    rf_feature_importances = pd.DataFrame({'Feature': processed_feature_names, 'Importance': rf_model.feature_importances_})\n",
        "    rf_feature_importances = rf_feature_importances.sort_values(by='Importance', ascending=False)\n",
        "    print(rf_feature_importances.head(10))\n",
        "\n",
        "    print(\"\\nXGBoost Feature Importance (Top 10):\")\n",
        "    xgb_feature_importances = pd.DataFrame({'Feature': processed_feature_names, 'Importance': xgb_model.feature_importances_})\n",
        "    xgb_feature_importances = xgb_feature_importances.sort_values(by='Importance', ascending=False)\n",
        "    print(xgb_feature_importances.head(10))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Step 20: Model Interpretability with SHAP (for Tuned XGBoost)  ---\n",
        "print(\"\\n--- Step 20:  Model Interpretability with SHAP (Tuned XGBoost) ---\")\n",
        "\n",
        "if not isinstance(processed_feature_names, list):\n",
        "    processed_feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n",
        "\n",
        "X_test_processed_df = pd.DataFrame(X_test_processed, columns=processed_feature_names)\n",
        "\n",
        "try:\n",
        "    explainer = shap.TreeExplainer(models['Tuned XGBoost'])\n",
        "    shap_values = explainer.shap_values(X_test_processed_df)\n",
        "\n",
        "    print(\"\\n SHAP Summary Plot (Global Feature Importance - Bar Plot)...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values, X_test_processed_df, plot_type=\"bar\", show=False)\n",
        "    plt.title('SHAP Feature Importance for Tuned XGBoost ')\n",
        "    plt.tight_layout()\n",
        "    plt.show() # Display plot\n",
        "    print(\"SHAP Summary Bar Plot displayed above.\")\n",
        "\n",
        "    print(\"\\nGenerating SHAP Summary Plot (Beeswarm Plot)...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values, X_test_processed_df, show=False) # Beeswarm plot\n",
        "    plt.title('SHAP Summary Plot (Beeswarm) for Tuned XGBoost ')\n",
        "    plt.tight_layout()\n",
        "    plt.show() #  plot\n",
        "\n",
        "    if 'age' in processed_feature_names:\n",
        "        print(\"\\nGenerating SHAP Dependence Plot for 'age'...\")\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        shap.dependence_plot(\"age\", shap_values, X_test_processed_df, show=False)\n",
        "        plt.title('SHAP Dependence Plot for Age ')\n",
        "        plt.tight_layout()\n",
        "        plt.show() # plot\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during SHAP analysis: {e}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SkSFg-jgAvgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Step 22: Feature Importances for All Models ---\n",
        "print(\"\\n--- Step 22: Feature Importances for All Models ---\")\n",
        "\n",
        "feature_importance_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            importance_vals = model.feature_importances_\n",
        "        elif hasattr(model, \"coef_\"):\n",
        "            importance_vals = np.abs(model.coef_[0])  # Logistic Regression\n",
        "        else:\n",
        "            # permutation importance\n",
        "            perm_result = permutation_importance(\n",
        "                model, X_test_processed, y_test,\n",
        "                n_repeats=10, random_state=42, n_jobs=-1\n",
        "            )\n",
        "            importance_vals = perm_result.importances_mean\n",
        "\n",
        "        feature_importance_results[name] = pd.DataFrame({\n",
        "            \"Feature\": processed_feature_names,\n",
        "            f\"Importance_{name}\": importance_vals\n",
        "        }).sort_values(by=f\"Importance_{name}\", ascending=False)\n",
        "\n",
        "        print(f\"\\nTop 10 Features for {name}:\")\n",
        "        print(feature_importance_results[name].head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping feature importance for {name}: {e}\")\n",
        "\n",
        "# --- Step 23: Combined Feature Importance Plot (Grouped Bars) ---\n",
        "print(\"\\n--- Step 23: Combined Feature Importance Plot ---\")\n",
        "\n",
        "# all importance tables\n",
        "merged_importances = feature_importance_results[list(feature_importance_results.keys())[0]][[\"Feature\"]]\n",
        "\n",
        "for name, df_imp in feature_importance_results.items():\n",
        "    merged_importances = merged_importances.merge(df_imp, on=\"Feature\", how=\"left\")\n",
        "\n",
        "# --- NEW: Scale all importance values per model to [0,1] ---\n",
        "scaler = MinMaxScaler()\n",
        "for col in merged_importances.columns:\n",
        "    if col != \"Feature\":\n",
        "        merged_importances[col] = scaler.fit_transform(merged_importances[[col]])\n",
        "\n",
        "# Melt for plotting\n",
        "merged_melted = pd.melt(\n",
        "    merged_importances,\n",
        "    id_vars=\"Feature\",\n",
        "    var_name=\"Model\",\n",
        "    value_name=\"Importance\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.barplot(data=merged_melted, x=\"Feature\", y=\"Importance\", hue=\"Model\")\n",
        "plt.title(\"Top Normalized Features across Models\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- Step 24: Calibration Curves ---\")\n",
        "\n",
        "# Define 12 distinct colors\n",
        "colors = [\n",
        "    \"#1f77b4\" ,\"#7f7788\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\",\n",
        "    \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\", \"#aec7e8\", \"#fcbb70\"\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i, (name, y_prob) in enumerate(prob_maps.items()):\n",
        "    color = colors[i % len(colors)]  # cycle colors if more than 12\n",
        "    try:\n",
        "        CalibrationDisplay.from_predictions(\n",
        "            y_test, y_prob, n_bins=10, strategy='uniform',\n",
        "            name=name, ax=plt.gca(), color=color\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping calibration for {name}: {e}\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly Calibrated\")\n",
        "plt.title(\"Calibration Curves for All Models\")\n",
        "plt.xlabel(\"Mean Predicted Probability\")\n",
        "plt.ylabel(\"Fraction of Positives\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yG_1uICVq4Lj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}